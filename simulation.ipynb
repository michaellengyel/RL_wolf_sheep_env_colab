{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ikoadXmwFOU"
   },
   "source": [
    "The Environment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37676,
     "status": "ok",
     "timestamp": 1578489658075,
     "user": {
      "displayName": "Michael Michael",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU6YaaA8lYGw9WMPWdiLSUyR02YDrlZyFI6G43=s64",
      "userId": "08834512687607198011"
     },
     "user_tz": -60
    },
    "id": "KaGOkrsJxe_p",
    "outputId": "dbd329f5-19a4-4bdc-e770-48d04671af31"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#/gdrive/My\\ Drive/Wolf_Sheep_Project/data/reduced_height_map.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGP9XFdsv2Bw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "class Environment:\n",
    "\n",
    "    # Initalize environment and sets agent to random location (s0, a0)\n",
    "    def __init__(self, map_img_path, fov, food_spawn_threshold, visuals=False):\n",
    "        # Private input variables\n",
    "        self.map_img_path = map_img_path\n",
    "        self.fov = fov\n",
    "        self.food_spawn_threshold = food_spawn_threshold\n",
    "        #self.visuals = visuals\n",
    "\n",
    "        # Private variables\n",
    "        self.offset = int((self.fov - 1) / 2)\n",
    "        self.map_img_agents = self.load_map()\n",
    "        self.map_img_calculations = self.load_map()\n",
    "        self.env_width = self.map_img_agents.shape[0]\n",
    "        self.env_height = self.map_img_agents.shape[1]\n",
    "        self.agent_x = 0\n",
    "        self.agent_y = 0\n",
    "        self.agent_reward = 0\n",
    "        self.agent_current_reward = 0\n",
    "        # Number of rewards randomly generated\n",
    "        self.no_of_rewards = 0\n",
    "        # Is the game done\n",
    "        self.done = False\n",
    "\n",
    "        # s0\n",
    "        self.init_map()\n",
    "        # a0\n",
    "        self.init_agent_pos()\n",
    "\n",
    "\n",
    "        cv2.namedWindow('MAP', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('MAP', 900, 900)\n",
    "        cv2.namedWindow('SUBMAP', cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow('SUBMAP', 400, 400)\n",
    "\n",
    "\n",
    "    ########## Environment Initialization Functions ##########\n",
    "\n",
    "    # Initialize position of agent randomly (a0)\n",
    "    def init_agent_pos(self):\n",
    "        rand_x = random.randrange(self.offset, self.env_width - self.offset)\n",
    "        rand_y = random.randrange(self.offset, self.env_height - self.offset)\n",
    "        self.agent_x = rand_x\n",
    "        self.agent_y = rand_y\n",
    "        self.map_img_agents[self.agent_x, self.agent_y, 0] = 255\n",
    "        #self.map_img[self.agent_x, self.agent_y, 1] = 0\n",
    "        #self.map_img[self.agent_x, self.agent_y, 2] = 0\n",
    "\n",
    "    # Load the map from a .png\n",
    "    def load_map(self):\n",
    "        img = plt.imread(self.map_img_path)\n",
    "        map_img = img.copy()\n",
    "        #map = torchvision.datasets.ImageFolder(root=map_path, transform=torchvision.transforms.ToTensor)\n",
    "        return map_img\n",
    "\n",
    "    # Spawn food\n",
    "    def init_map(self):\n",
    "\n",
    "        # Iterate through x dim\n",
    "        for i in range(self.offset, self.env_width - self.offset):\n",
    "            # Iterate through y dim\n",
    "            for j in range(self.offset, self.env_height - self.offset):\n",
    "                # Generate a number in the range of the pixel at i, j\n",
    "                rand = random.randrange(0, self.map_img_agents[i, j, 0])\n",
    "                # If the generated number is 0 and pixel i, j is sub 30\n",
    "                if ((self.map_img_agents[i, j, 0] < self.food_spawn_threshold) & (rand <= 3)):\n",
    "                    # Set pixel red\n",
    "                    self.map_img_agents[i, j, 0] = 0\n",
    "                    self.map_img_agents[i, j, 1] = 255\n",
    "                    self.map_img_agents[i, j, 2] = 0\n",
    "                    # Increment the number of rewards counter\n",
    "                    self.no_of_rewards += 1\n",
    "                #print(self.map_img[i, j ,2])\n",
    "\n",
    "\n",
    "    ########## Reset Function ##########\n",
    "\n",
    "    def reset(self):\n",
    "        self.map_img_agents = self.load_map()\n",
    "        self.map_img_calculations = self.load_map()\n",
    "        self.init_agent_pos()\n",
    "        self.init_map()\n",
    "\n",
    "        self.sub_map_img = self.map_img_agents[self.agent_x - self.offset:self.agent_x + self.offset + 1, self.agent_y - self.offset:self.agent_y + self.offset + 1, :]\n",
    "        return self.sub_map_img\n",
    "\n",
    "    ########## Getter Function ##########\n",
    "\n",
    "    ########## Debug Helper Functions ##########\n",
    "\n",
    "    # Might need to make a torch.tensor from the numpy array\n",
    "    def make_tensor_from_image(self):\n",
    "        print(\"Making torch.tensor from image\")\n",
    "\n",
    "    def print_map_info(self):\n",
    "        print(type(self.map_img_agents))\n",
    "        print(self.map_img_agents.shape)\n",
    "\n",
    "    def print_agent_info(self):\n",
    "        print(self.agent_y, self.agent_x)\n",
    "\n",
    "    ########## GUI Render Functions ##########\n",
    "\n",
    "    def render_map(self):\n",
    "        cv2.imshow('MAP', self.map_img_agents)\n",
    "\n",
    "    # Render the map of the environment each tick\n",
    "    def render_sub_map(self):\n",
    "        cv2.imshow('SUBMAP', self.sub_map_img)\n",
    "\n",
    "    ########## Environment Logic Functions ##########\n",
    "\n",
    "    def movement_decision(self, x, y):\n",
    "\n",
    "        lower_inside_bounds_x = ((self.agent_x + x) > self.offset)\n",
    "        lower_inside_bounds_y = ((self.agent_y + y) > self.offset)\n",
    "        upper_inside_bounds_x = ((self.agent_x + x) < self.env_width - self.offset)\n",
    "        upper_inside_bounds_y = ((self.agent_y + y) < self.env_height - self.offset)\n",
    "\n",
    "        if ((lower_inside_bounds_x) & (lower_inside_bounds_y) & upper_inside_bounds_x & upper_inside_bounds_y):\n",
    "            # Un-paint the agent\n",
    "            self.map_img_agents[self.agent_x, self.agent_y, 0] = self.map_img_calculations[self.agent_x, self.agent_y, 0]\n",
    "            self.map_img_agents[self.agent_x, self.agent_y, 1] = self.map_img_calculations[self.agent_x, self.agent_y, 1]\n",
    "            self.map_img_agents[self.agent_x, self.agent_y, 2] = self.map_img_calculations[self.agent_x, self.agent_y, 2]\n",
    "            # Update the agent's position\n",
    "            self.agent_x += x\n",
    "            self.agent_y += y\n",
    "            # Paint the agent to it's new position\n",
    "            self.map_img_agents[self.agent_x, self.agent_y, 0] = 255\n",
    "            self.map_img_agents[self.agent_x, self.agent_y, 1] = 0\n",
    "            self.map_img_agents[self.agent_x, self.agent_y, 2] = 0\n",
    "        #else:\n",
    "            #print(\"Out of bounds\")\n",
    "\n",
    "    # Calculating net reward\n",
    "    def calculate_reward(self, x, y):\n",
    "        # Logic for gaining rewards\n",
    "        if ((self.map_img_agents[self.agent_x + x, self.agent_y + y, 1]) == 255):\n",
    "            self.agent_reward += 1\n",
    "            #print(\"Reward: \", self.agent_reward)\n",
    "        else:\n",
    "            self.agent_reward -= 0\n",
    "\n",
    "    # Calculating step's reward\n",
    "    def calculate_current_reward(self, x, y):\n",
    "        # Logic for gaining rewards\n",
    "        # Reset current reward to 0\n",
    "        self.agent_current_reward = 0\n",
    "        # If stepped on reward set current reward to 1\n",
    "        if ((self.map_img_agents[self.agent_x + x, self.agent_y + y, 1]) == 255):\n",
    "            self.agent_current_reward = 1\n",
    "            #print(\"Reward: \", self.agent_current_reward)\n",
    "        else:\n",
    "            self.agent_current_reward = 0\n",
    "\n",
    "    # Update the environment based on the action\n",
    "    def step(self, action):\n",
    "\n",
    "        # UP\n",
    "        if (action == 0):\n",
    "            #print(\"UP\")\n",
    "            self.calculate_reward(0, -1)\n",
    "            self.calculate_current_reward(0, -1)\n",
    "            self.movement_decision(0, -1)\n",
    "        # DOWN\n",
    "        elif (action == 1):\n",
    "            #print(\"DOWN\")\n",
    "            self.calculate_reward(0, 1)\n",
    "            self.calculate_current_reward(0, 1)\n",
    "            self.movement_decision(0, 1)\n",
    "        # LEFT\n",
    "        elif (action == 2):\n",
    "            #print(\"LEFT\")\n",
    "            self.calculate_reward(-1, 0)\n",
    "            self.calculate_current_reward(-1, 0)\n",
    "            self.movement_decision(-1, 0)\n",
    "        # RIGHT\n",
    "        elif (action == 3):\n",
    "            #print(\"RIGHT\")\n",
    "            self.calculate_reward(1, 0)\n",
    "            self.calculate_current_reward(1, 0)\n",
    "            self.movement_decision(1, 0)\n",
    "        # UP/LEFT\n",
    "        elif (action == 4):\n",
    "            #print(\"UP/LEFT\")\n",
    "            self.calculate_reward(-1, -1)\n",
    "            self.calculate_current_reward(-1, -1)\n",
    "            self.movement_decision(-1, -1)\n",
    "        # UP/RIGHT\n",
    "        elif (action == 5):\n",
    "            #print(\"UP/RIGHT\")\n",
    "            self.calculate_reward(1, -1)\n",
    "            self.calculate_current_reward(1, -1)\n",
    "            self.movement_decision(1, -1)\n",
    "        # DOWN/LEFT\n",
    "        elif (action == 6):\n",
    "            #print(\"DOWN/LEFT\")\n",
    "            self.calculate_reward(-1, 1)\n",
    "            self.calculate_current_reward(-1, 1)\n",
    "            self.movement_decision(-1, 1)\n",
    "        # DOWN/RIGHT\n",
    "        elif (action == 7):\n",
    "            #print(\"DOWN/RIGHT\")\n",
    "            self.calculate_reward(1, 1)\n",
    "            self.calculate_current_reward(1, 1)\n",
    "            self.movement_decision(1, 1)\n",
    "\n",
    "        # Check if all rewards have been found, if yes set done to true\n",
    "\n",
    "        if((self.no_of_rewards/2) <= self.agent_reward):\n",
    "            self.done = True\n",
    "\n",
    "        # Crop the submap from the map\n",
    "        self.sub_map_img = self.map_img_agents[self.agent_x - self.offset:self.agent_x + self.offset + 1, self.agent_y - self.offset:self.agent_y + self.offset + 1, :]\n",
    "\n",
    "        return self.sub_map_img, self.agent_current_reward, self.done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsOFd5Kov3k_"
   },
   "source": [
    "The Agent Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43792,
     "status": "ok",
     "timestamp": 1578489664246,
     "user": {
      "displayName": "Michael Michael",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU6YaaA8lYGw9WMPWdiLSUyR02YDrlZyFI6G43=s64",
      "userId": "08834512687607198011"
     },
     "user_tz": -60
    },
    "id": "Ax_kXK1K990O",
    "outputId": "f671b86e-df09-40e1-9a9d-7e56aabb299e"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from skimage import transform           # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray      # Help us to gray our frames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEJwGI1J990U"
   },
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Grayscale frame\n",
    "    # gray = rgb2gray(frame)\n",
    "\n",
    "    # Crop the screen\n",
    "    # [Up:Down, Left:Right]\n",
    "    # cropped_frame = gray[:, :]\n",
    "\n",
    "    # Normalized Pixel Values\n",
    "    normalized_frame = frame/255.0\n",
    "\n",
    "    # Resize\n",
    "    #preprocess_frame = transform.resize(normalized_frame, [15, 15])\n",
    "\n",
    "    # Return 15x15x1\n",
    "    return normalized_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nwyMdlX990X"
   },
   "outputs": [],
   "source": [
    "def stack_framesX(stacked_frames, state, is_new_episode, stack_size):\n",
    "    # Preproccess frame\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        # Clear out stacked_frames\n",
    "        stacked_frames = deque([np.zeros((15, 15), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "        # Because we are in a new episode, copy the same frame x4\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4edh7wfD990a"
   },
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions, sess, dqnetwork):\n",
    "    ## EPSILON GREEDY STRATAGY\n",
    "    # Choose action a from state s using epsilon greedy\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we\"ll use an improved version of out epsilon greedy stratagy used in Q-Learning notebook\n",
    "    # Here we\"ll use an improved version of out epsilon greedy stratagy used in Q-Learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if explore_probability > exp_exp_tradeoff:\n",
    "        # Make a random action (exploration)\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        Qs = sess.run(dqnetwork.output, feed_dict={dqnetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xr36lfYw990d"
   },
   "outputs": [],
   "source": [
    "def onehot_to_int(action):\n",
    "    return np.argmax(action, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-LZsSSi990i"
   },
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='dqnetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each element of a state_size in tuple\n",
    "\n",
    "            # [None, 15, 15, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name=\"actions\")\n",
    "\n",
    "            # Remember that target_q is the R(s, a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "\n",
    "            '''\n",
    "            First convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            # Input is 15x15x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=8,\n",
    "                                          kernel_size=[4, 4],\n",
    "                                          strides=[1, 1],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv1\")\n",
    "\n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "\n",
    "            '''\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            # Input is 15x15x4\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=16,\n",
    "                                          kernel_size=[3, 3],\n",
    "                                          strides=[1, 1],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv2\")\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "\n",
    "            '''\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            # Input is 15x15x4=900\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=16,\n",
    "                                          kernel_size=[3, 3],\n",
    "                                          strides=[1, 1],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv3\")\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "\n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "\n",
    "            self.fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=70,\n",
    "                                      activation=tf.nn.elu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc1\")\n",
    "\n",
    "            self.output = tf.layers.dense(inputs=self.fc,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units=self.action_size,\n",
    "                                          activation=None)\n",
    "\n",
    "            # Q is out predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "\n",
    "            # The Loss is the difference between out predicted Q_values and Q_target\n",
    "            # Sum (Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgdWZcfb990l"
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=batch_size,\n",
    "                                 replace=False)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1115,
     "status": "ok",
     "timestamp": 1578490302899,
     "user": {
      "displayName": "Michael Michael",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAU6YaaA8lYGw9WMPWdiLSUyR02YDrlZyFI6G43=s64",
      "userId": "08834512687607198011"
     },
     "user_tz": -60
    },
    "id": "65-XRiZ9990o",
    "outputId": "c463ff29-128c-45bd-d268-32ba2ec6a4f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-8-42a553cffabb>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-8-42a553cffabb>:72: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Actions taken: 5000\n",
      "Actions taken: 10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-45804e8e56b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    218\u001b[0m                                    feed_dict={dqnetwork.inputs_: states_mb,\n\u001b[0;32m    219\u001b[0m                                               \u001b[0mdqnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets_mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                               dqnetwork.actions_: actions_mb})\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;31m# Write TF Summaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Env size is 15x15x3\n",
    "#env = Environment(\"/content/gdrive/My Drive/Wolf_Sheep_Project/RL_wolf_sheep_env_colab/data/reduced_height_map.jpg\", 15, 100, True)\n",
    "env = Environment(\"data/reduced_height_map.jpg\", 15, 100, True)\n",
    "\n",
    "# Hardcoded action space: 8\n",
    "action_space = 8\n",
    "\n",
    "# One-Hot encoded actions\n",
    "possible_actions = np.array(np.identity(action_space, dtype=int)).tolist()\n",
    "\n",
    "# Number of frames we stack\n",
    "stack_size = 4\n",
    "\n",
    "# Init Deque with zero-images one array for each image\n",
    "stacked_frames = deque([np.zeros((15, 15), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [15, 15, 4]\n",
    "action_size = 8\n",
    "learning_rate = 0.00025\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 2\n",
    "max_steps = 30000\n",
    "batch_size = 64\n",
    "\n",
    "# Exploration parameters for epsilon greedy stratagy\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.00001\n",
    "\n",
    "# Q Learning hyperparameters\n",
    "gamma = 0.9\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size\n",
    "memory_size = 1000000\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = True\n",
    "\n",
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "dqnetwork = DQNetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "# Instantiate memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        state = env.reset() # TODO: Change this to work with WS env\n",
    "        \n",
    "        state = preprocess_frame(state)\n",
    "        #state, stacked_frames = stack_framesX(stacked_frames, state, False, stack_size)\n",
    "\n",
    "    # Get the next_state, the rewards, done by tacking a random action\n",
    "    choice = random.randint(1, len(possible_actions))-1\n",
    "    action = possible_actions[choice]\n",
    "    next_state, reward, done = env.step(onehot_to_int(action))\n",
    "\n",
    "    if (episode_render == True):\n",
    "        env.render_map()\n",
    "        env.render_sub_map()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    #Stack the frames\n",
    "    state = preprocess_frame(state)\n",
    "    #next_state, stacked_frames = stack_framesX(stacked_frames, next_state, False, stack_size)\n",
    "\n",
    "    # If the episode is finished (we'r dead 3x)\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "\n",
    "        # Add experiance to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        # Start a new episode\n",
    "        state = env.reset() # TODO: change to be able to reset the env\n",
    "\n",
    "        # Stack the frames\n",
    "        state = preprocess_frame(state)\n",
    "        #state, stacked_frames = stack_framesX(stacked_frames, state, True, stack_size)\n",
    "\n",
    "    else:\n",
    "        # Add experiance to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        # Our new state is new the next_state\n",
    "        state = next_state\n",
    "\n",
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "# Losses\n",
    "tf.summary.scalar(\"Loss\", dqnetwork.loss)\n",
    "#tf.summary.scalar(\"Loss\", dqnetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()\n",
    "\n",
    "# Saver will help us to save out model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Initialize the decay rate (that will be used to reduce epsilon)\n",
    "        decay_step = 0\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "\n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            rewards_list = []\n",
    "\n",
    "\n",
    "            # Make a new episode and observe the first state\n",
    "            state = env.reset() # TODO: implement reset function in Environment\n",
    "\n",
    "            # Remember that stack frame function also calls our preprocess function.\n",
    "            state = preprocess_frame(state)\n",
    "            #state, stacked_frames = stack_framesX(stacked_frames, state, True, stack_size)\n",
    "\n",
    "            while (step < max_steps):\n",
    "                step += 1\n",
    "\n",
    "                # Increase decay_step\n",
    "                decay_step += 1\n",
    "\n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step,\n",
    "                                                             state, possible_actions, sess, dqnetwork)\n",
    "\n",
    "                # Perform the action and get the next_state, reward and done info\n",
    "                next_state, reward, done = env.step(onehot_to_int(action)) # TODO: some changed may needed here (changes done in environment.py)\n",
    "\n",
    "                if (episode_render == True):\n",
    "                    env.render_map()\n",
    "                    env.render_sub_map()\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "                # Add the reward to the total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # The episode ends so no next state\n",
    "                    next_state = np.zeros((15, 15), dtype=np.int)\n",
    "\n",
    "                    #next_state, stack_frames = stack_framesX(stacked_frames, next_state, False, stack_size)\n",
    "                    next_state = preprocess_frame(next_state)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                          'total Reward: {}'.format(total_reward),\n",
    "                          'Explore P: {:.4f}'.format(explore_probability),\n",
    "                          'Training Loss {:.4f}'.format(loss))\n",
    "\n",
    "                    rewards_list.append((episode, total_reward))\n",
    "\n",
    "                    # Store transition <st, at, rt+1, st+1> in memory D\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Stack the frame of the next_state\n",
    "                    #next_state, stacked_frames = stack_framesX(stacked_frames, next_state, False, stack_size)\n",
    "                    next_state = preprocess_frame(next_state)\n",
    "\n",
    "                    # Add experiance to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                    # st+1 is now out current state\n",
    "                    state = next_state\n",
    "\n",
    "                ### LEARNING PART\n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # get Q values for next_state\n",
    "                Qs_next_state = sess.run(dqnetwork.output, feed_dict={dqnetwork.inputs_: next_states_mb})\n",
    "\n",
    "                # Set Q_ target = r is the episode ends at s+1, otherwise set Q_ target = r + gamma*maxQ(s',a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([dqnetwork.loss, dqnetwork.optimizer],\n",
    "                                   feed_dict={dqnetwork.inputs_: states_mb,\n",
    "                                              dqnetwork.target_Q: targets_mb,\n",
    "                                              dqnetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={dqnetwork.inputs_: states_mb,\n",
    "                                                        dqnetwork.target_Q: targets_mb,\n",
    "                                                        dqnetwork.actions_: actions_mb})\n",
    "\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                if step % 5000 == 0:\n",
    "                    print(\"Actions taken: {}\" .format(step))\n",
    "\n",
    "            # Save model every 1 episodes\n",
    "            if episode % 1 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                #save_path = saver.save(sess, \"/content/gdrive/My Drive/Wolf_Sheep_Project/RL_wolf_sheep_env_colab/models/model.ckpt\")\n",
    "                print(\"Model Saved: {}\" .format(step), \"{}\" .format(episode))\n",
    "    \n",
    "    print(\"Training Finished\")\n",
    "\n",
    "\n",
    "if training == False:\n",
    "    print(\"Inference Starting\")\n",
    "    with tf.Session() as sess:\n",
    "        total_test_rewards = []\n",
    "\n",
    "        # Lead the model\n",
    "        saver.restore(sess, \"./models/model.ckpt\")\n",
    "        #saver.restore(sess, \"/content/gdrive/My Drive/Wolf_Sheep_Project/RL_wolf_sheep_env_colab/models/model.ckpt\")\n",
    "\n",
    "        for episode in range(1):\n",
    "            env.reset()\n",
    "            state, reward, done = env.step(3)\n",
    "            #state, stacked_frames = stack_framesX(stacked_frames, state, True, stack_size)\n",
    "            state = preprocess_frame(state)\n",
    "\n",
    "            print(\"****************************************************\")\n",
    "            print(\"EPISODE \", episode)\n",
    "\n",
    "            while True:\n",
    "                # Reshape the state\n",
    "                state = state.reshape((1, *state_size))\n",
    "                # Get action from Q-network\n",
    "                # Estimate the Qs values state\n",
    "                Qs = sess.run(dqnetwork.output, feed_dict={dqnetwork.inputs_: state})\n",
    "\n",
    "                # Take the biggest Q value (= the best action)\n",
    "                choice = np.argmax(Qs)\n",
    "                action = possible_actions[choice]\n",
    "\n",
    "                print(onehot_to_int(action))\n",
    "\n",
    "                # Perform the action and get the next_state, reward, and done information\n",
    "                next_state, reward, done = env.step(onehot_to_int(action))\n",
    "\n",
    "                if (episode_render == True):\n",
    "                    env.render_map()\n",
    "                    env.render_sub_map()\n",
    "                    if cv2.waitKey(1000) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "                #total_rewards += reward\n",
    "\n",
    "                '''\n",
    "                if done:\n",
    "                    print(\"Score\", total_rewards)\n",
    "                    total_test_rewards.append(total_rewards)\n",
    "                    break\n",
    "                '''\n",
    "\n",
    "                next_state = preprocess_frame(next_state)\n",
    "                state = next_state\n",
    "\n",
    "    print(\"Inference Finished\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "simulation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
