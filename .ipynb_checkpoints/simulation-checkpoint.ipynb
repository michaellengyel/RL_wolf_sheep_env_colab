{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from skimage import transform           # Help us to preprocess the frames\n",
    "from skimage.color import rgb2gray      # Help us to gray our frames\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Grayscale frame\n",
    "    gray = rgb2gray(frame)\n",
    "\n",
    "    # Crop the screen\n",
    "    # [Up:Down, Left:Right]\n",
    "    cropped_frame = gray[:, :]\n",
    "\n",
    "    # Normalized Pixel Values\n",
    "    normalized_frame = cropped_frame/255.0\n",
    "\n",
    "    # Resize\n",
    "    preprocess_frame = transform.resize(normalized_frame, [15, 15])\n",
    "\n",
    "    # Return 15x15x1\n",
    "    return preprocess_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_framesX(stacked_frames, state, is_new_episode, stack_size):\n",
    "    # Preproccess frame\n",
    "    frame = preprocess_frame(state)\n",
    "\n",
    "    if is_new_episode:\n",
    "        # Clear out stacked_frames\n",
    "        stacked_frames = deque([np.zeros((15, 15), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "        # Because we are in a new episode, copy the same frame x4\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Stack the frames\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stacked_frames.append(frame)\n",
    "\n",
    "        # Build the stacked state (first dimension specifies different frames)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "\n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions, sess, dqnetwork):\n",
    "    ## EPSILON GREEDY STRATAGY\n",
    "    # Choose action a from state s using epsilon greedy\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "\n",
    "    # Here we\"ll use an improved version of out epsilon greedy stratagy used in Q-Learning notebook\n",
    "    # Here we\"ll use an improved version of out epsilon greedy stratagy used in Q-Learning notebook\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if explore_probability > exp_exp_tradeoff:\n",
    "        # Make a random action (exploration)\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "\n",
    "    else:\n",
    "        # Get action from Q-network (exploitation)\n",
    "        Qs = sess.run(dqnetwork.output, feed_dict={dqnetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "\n",
    "        # Take the biggest Q value (= the best action)\n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "\n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_int(action):\n",
    "    return np.argmax(action, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='dqnetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # We create the placeholders\n",
    "            # *state_size means that we take each element of a state_size in tuple\n",
    "\n",
    "            # [None, 15, 15, 4]\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name=\"actions\")\n",
    "\n",
    "            # Remember that target_q is the R(s, a) + ymax Qhat(s', a')\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "\n",
    "            '''\n",
    "            First convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            # Input is 15x15x4\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.inputs_,\n",
    "                                          filters=8,\n",
    "                                          kernel_size=[4, 4],\n",
    "                                          strides=[1, 1],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv1\")\n",
    "\n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name=\"conv1_out\")\n",
    "\n",
    "            '''\n",
    "            Second convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            # Input is 15x15x4\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.conv1_out,\n",
    "                                          filters=16,\n",
    "                                          kernel_size=[3, 3],\n",
    "                                          strides=[1, 1],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv2\")\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name=\"conv2_out\")\n",
    "\n",
    "            '''\n",
    "            Third convnet:\n",
    "            CNN\n",
    "            ELU\n",
    "            '''\n",
    "            # Input is 15x15x4=900\n",
    "            self.conv3 = tf.layers.conv2d(inputs=self.conv2_out,\n",
    "                                          filters=16,\n",
    "                                          kernel_size=[3, 3],\n",
    "                                          strides=[1, 1],\n",
    "                                          padding=\"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                          name=\"conv3\")\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name=\"conv3_out\")\n",
    "\n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "\n",
    "            self.fc = tf.layers.dense(inputs=self.flatten,\n",
    "                                      units=70,\n",
    "                                      activation=tf.nn.elu,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      name=\"fc1\")\n",
    "\n",
    "            self.output = tf.layers.dense(inputs=self.fc,\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units=self.action_size,\n",
    "                                          activation=None)\n",
    "\n",
    "            # Q is out predicted Q value.\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_))\n",
    "\n",
    "            # The Loss is the difference between out predicted Q_values and Q_target\n",
    "            # Sum (Qtarget - Q)^2\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=batch_size,\n",
    "                                 replace=False)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-6-42a553cffabb>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv2d instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Michael\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-6-42a553cffabb>:72: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "Actions taken: 1000\n",
      "Actions taken: 2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8e9637bc09c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    217\u001b[0m                                    feed_dict={dqnetwork.inputs_: states_mb,\n\u001b[0;32m    218\u001b[0m                                               \u001b[0mdqnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_Q\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets_mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m                                               dqnetwork.actions_: actions_mb})\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m                 \u001b[1;31m# Write TF Summaries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Env size is 15x15x3\n",
    "env = Environment(\"data/reduced_height_map.jpg\", 15, 50, True)\n",
    "\n",
    "# Hardcoded action space: 8\n",
    "action_space = 8\n",
    "\n",
    "# One-Hot encoded actions\n",
    "possible_actions = np.array(np.identity(action_space, dtype=int)).tolist()\n",
    "\n",
    "# Number of frames we stack\n",
    "stack_size = 4\n",
    "\n",
    "# Init Deque with zero-images one array for each image\n",
    "stacked_frames = deque([np.zeros((15, 15), dtype=np.int) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "### MODEL HYPERPARAMETERS\n",
    "state_size = [15, 15, 4]\n",
    "action_size = 8\n",
    "learning_rate = 0.00025\n",
    "\n",
    "### TRAINING HYPERPARAMETERS\n",
    "total_episodes = 50\n",
    "max_steps = 30000\n",
    "batch_size = 64\n",
    "\n",
    "# Exploration parameters for epsilon greedy stratagy\n",
    "explore_start = 1.0\n",
    "explore_stop = 0.01\n",
    "decay_rate = 0.00001\n",
    "\n",
    "# Q Learning hyperparameters\n",
    "gamma = 0.9\n",
    "\n",
    "### MEMORY HYPERPARAMETERS\n",
    "pretrain_length = batch_size\n",
    "memory_size = 1000000\n",
    "\n",
    "### PREPROCESSING HYPERPARAMETERS\n",
    "stack_size = 4\n",
    "\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\n",
    "training = True\n",
    "\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\n",
    "episode_render = False\n",
    "\n",
    "# Reset the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Instantiate the DQNetwork\n",
    "dqnetwork = DQNetwork(state_size, action_size, learning_rate)\n",
    "\n",
    "# Instantiate memory\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    # If it's the first step\n",
    "    if i == 0:\n",
    "        state = env.reset() # TODO: Change this to work with WS env\n",
    "\n",
    "        state, stacked_frames = stack_framesX(stacked_frames, state, False, stack_size)\n",
    "\n",
    "    # Get the next_state, the rewards, done by tacking a random action\n",
    "    choice = random.randint(1, len(possible_actions))-1\n",
    "    action = possible_actions[choice]\n",
    "    next_state, reward, done = env.step(onehot_to_int(action))\n",
    "\n",
    "    if (episode_render == True):\n",
    "        env.render_map()\n",
    "        env.render_sub_map()\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    #Stack the frames\n",
    "    next_state, stacked_frames = stack_framesX(stacked_frames, next_state, False, stack_size)\n",
    "\n",
    "    # If the episode is finished (we'r dead 3x)\n",
    "    if done:\n",
    "        # We finished the episode\n",
    "        next_state = np.zeros(state.shape)\n",
    "\n",
    "        # Add experiance to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        # Start a new episode\n",
    "        state = env.reset() # TODO: change to be able to reset the env\n",
    "\n",
    "        # Stack the frames\n",
    "        state, stacked_frames = stack_framesX(stacked_frames, state, True, stack_size)\n",
    "\n",
    "    else:\n",
    "        # Add experiance to memory\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "        # Our new state is new the next_state\n",
    "        state = next_state\n",
    "\n",
    "# Setup TensorBoard Writer\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "# Losses\n",
    "tf.summary.scalar(\"Loss\", dqnetwork.loss)\n",
    "#tf.summary.scalar(\"Loss\", dqnetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()\n",
    "\n",
    "# Saver will help us to save out model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize the variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Initialize the decay rate (that will be used to reduce epsilon)\n",
    "        decay_step = 0\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            # Set step to 0\n",
    "            step = 0\n",
    "\n",
    "            # Initialize the rewards of the episode\n",
    "            episode_rewards = []\n",
    "            rewards_list = []\n",
    "\n",
    "\n",
    "            # Make a new episode and observe the first state\n",
    "            state = env.reset() # TODO: implement reset function in Environment\n",
    "\n",
    "            # Remember that stack frame function also calls our preprocess function.\n",
    "            state, stacked_frames = stack_framesX(stacked_frames, state, True, stack_size)\n",
    "\n",
    "            while (step < max_steps):\n",
    "                step += 1\n",
    "\n",
    "                # Increase decay_step\n",
    "                decay_step += 1\n",
    "\n",
    "                # Predict the action to take and take it\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step,\n",
    "                                                             state, possible_actions, sess, dqnetwork)\n",
    "\n",
    "                # Perform the action and get the next_state, reward and done info\n",
    "                next_state, reward, done = env.step(onehot_to_int(action)) # TODO: some changed may needed here (changes done in environment.py)\n",
    "\n",
    "                if (episode_render == True):\n",
    "                    env.render_map()\n",
    "                    env.render_sub_map()\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "                # Add the reward to the total reward\n",
    "                episode_rewards.append(reward)\n",
    "\n",
    "                # If the game is finished\n",
    "                if done:\n",
    "                    # The episode ends so no next state\n",
    "                    next_state = np.zeros((15, 15), dtype=np.int)\n",
    "\n",
    "                    next_state, stack_frames = stack_framesX(stacked_frames, next_state, False, stack_size)\n",
    "\n",
    "                    # Set step = max_steps to end the episode\n",
    "                    step = max_steps\n",
    "\n",
    "                    # Get the total reward of the episode\n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "\n",
    "                    print('Episode: {}'.format(episode),\n",
    "                          'total Reward: {}'.format(total_reward),\n",
    "                          'Explore P: {:.4f}'.format(explore_probability),\n",
    "                          'Training Loss {:.4f}'.format(loss))\n",
    "\n",
    "                    rewards_list.append((episode, total_reward))\n",
    "\n",
    "                    # Store transition <st, at, rt+1, st+1> in memory D\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                else:\n",
    "                    # Stack the frame of the next_state\n",
    "                    next_state, stacked_frames = stack_framesX(stacked_frames, next_state, False, stack_size)\n",
    "\n",
    "                    # Add experiance to memory\n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "\n",
    "                    # st+1 is now out current state\n",
    "                    state = next_state\n",
    "\n",
    "                ### LEARNING PART\n",
    "                # Obtain random mini-batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "\n",
    "                target_Qs_batch = []\n",
    "\n",
    "                # get Q values for next_state\n",
    "                Qs_next_state = sess.run(dqnetwork.output, feed_dict={dqnetwork.inputs_: next_states_mb})\n",
    "\n",
    "                # Set Q_ target = r is the episode ends at s+1, otherwise set Q_ target = r + gamma*maxQ(s',a')\n",
    "                for i in range(0, len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "\n",
    "                    # If we are in a terminal state, only equals reward\n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "\n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "\n",
    "                loss, _ = sess.run([dqnetwork.loss, dqnetwork.optimizer],\n",
    "                                   feed_dict={dqnetwork.inputs_: states_mb,\n",
    "                                              dqnetwork.target_Q: targets_mb,\n",
    "                                              dqnetwork.actions_: actions_mb})\n",
    "\n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op, feed_dict={dqnetwork.inputs_: states_mb,\n",
    "                                                        dqnetwork.target_Q: targets_mb,\n",
    "                                                        dqnetwork.actions_: actions_mb})\n",
    "\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                if step % 1000 == 0:\n",
    "                    print(\"Actions taken: {}\" .format(step))\n",
    "\n",
    "            # Save model every 1 episodes\n",
    "            if episode % 1 == 0:\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\n",
    "                print(\"Model Saved: {}\" .format(step), \"{}\" .format(episode))\n",
    "\n",
    "print(\"Inference Starting\")\n",
    "with tf.Session() as sess:\n",
    "    total_test_rewards = []\n",
    "\n",
    "    # Lead the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(1):\n",
    "        env.reset()\n",
    "        state, reward, done = env.step(3)\n",
    "        state, stacked_frames = stack_framesX(stacked_frames, state, True, stack_size)\n",
    "\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "            # Reshape the state\n",
    "            state = state.reshape((1, *state_size))\n",
    "            # Get action from Q-network\n",
    "            # Estimate the Qs values state\n",
    "            Qs = sess.run(dqnetwork.output, feed_dict={dqnetwork.inputs_: state})\n",
    "\n",
    "            # Take the biggest Q value (= the best action)\n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[choice]\n",
    "\n",
    "            print(onehot_to_int(action))\n",
    "\n",
    "            # Perform the action and get the next_state, reward, and done information\n",
    "            next_state, reward, done = env.step(onehot_to_int(action))\n",
    "\n",
    "            if (episode_render == True):\n",
    "                env.render_map()\n",
    "                env.render_sub_map()\n",
    "                if cv2.waitKey(100) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "            #total_rewards += reward\n",
    "\n",
    "            '''\n",
    "            if done:\n",
    "                print(\"Score\", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "            '''\n",
    "\n",
    "            next_state, stacked_frames = stack_framesX(stacked_frames, next_state, False, stack_size)\n",
    "            state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
